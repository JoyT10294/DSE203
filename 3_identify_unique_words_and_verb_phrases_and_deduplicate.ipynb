{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cd48e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joythompson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joythompson/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "from pandas import DataFrame\n",
    "import itertools\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests\n",
    "import random\n",
    "import torch\n",
    "import xmltodict\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml\n",
    "from xml.etree.ElementTree import parse\n",
    "from xml.parsers import expat\n",
    "from xml.dom.minidom import parse, parseString\n",
    "import psycopg2\n",
    "import PyPDF2\n",
    "import pikepdf\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "username='joythompson'\n",
    "directorybillhead=\"/Users/\"+username+\"/Desktop/DSE_203_JoyT/DSE_203_Final_Project_JoyT/Bill_Parsing_Headers/\"\n",
    "headers_v2=pd.read_csv(directorybillhead+ 'header_text_by_bill_header_with_order.csv')[0:50000].drop(columns=['Unnamed: 0', 'BillName', 'Header', 'HeaderOrderID', 'Key'])\n",
    "headers_v2['Text']=headers_v2.groupby(['BillID', 'HeaderID'])['Text'].transform(lambda x : ' '.join(x))\n",
    "header_gb=headers_v2.dropna(how='any').drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23690357",
   "metadata": {},
   "source": [
    "# **Identify Unique & Categorized Nouns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6bcc43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findkeywordlabel(headers):\n",
    "    uniquewordpd=pd.DataFrame(columns=['WordText','WordTextLower', 'CategoryLabel']);\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    wordlabelpd=pd.DataFrame(columns=['BillID', 'HeaderID', 'EntityOrderID', 'Text', 'CategoryLabel']);\n",
    "    billidlist=[];\n",
    "    for ind in headers.index:\n",
    "        doc = nlp(str(headers.loc[ind,'Text']).replace('-', ' ').replace('     ', ''));\n",
    "        entorder=0;\n",
    "        for ent in doc.ents:\n",
    "            entorder=entorder+1;\n",
    "            if ent.text.find('Act')!=-1 or ent.text.find('ACT')!=-1:\n",
    "                label='ACT';\n",
    "            else:\n",
    "                label=ent.label_;\n",
    "            wordlabelpd=wordlabelpd.append({'BillID':headers.loc[ind, 'BillID'], 'HeaderID':headers.loc[ind, 'HeaderID'],\n",
    "                                            'EntityOrderID':entorder , 'HeaderText': ent.text, 'CategoryLabel': label}, ignore_index=True);\n",
    "            if ent.text.lower() not in uniquewordpd['WordTextLower'].values:\n",
    "                uniquewordpd=uniquewordpd.append({'WordText': ent.text, 'WordTextLower': ent.text.lower(), 'CategoryLabel':label}, ignore_index=True);\n",
    "        if str(headers.loc[ind, 'BillID'])[-1]=='0' and headers.loc[ind, 'BillID'] not in billidlist:\n",
    "            billidlist.append(headers.loc[ind, 'BillID']);\n",
    "    return wordlabelpd, uniquewordpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a5ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "noungroupspd_v2, uniquenounspd_v2=findkeywordlabel(header_gb)\n",
    "noungroupspd_v2.to_csv(directorybillhead+ 'nouns_by_bill_header_with_groups_v2.csv')\n",
    "uniquenounspd_v2.to_csv(directorybillhead+ 'unique_nouns_with_groups_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a125c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlabelpd2, specialwordpd=findkeywordlabel(header_gb)\n",
    "wordlabelpd2.to_csv(directorybillhead+ 'nouns_by_bill_header_with_groups_v3.csv')\n",
    "specialwordpd.to_csv(directorybillhead+ 'unique_nouns_with_groups_v3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eb0725",
   "metadata": {},
   "source": [
    "# **Deduplicate Unique & Categorized Nouns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o', 'p', 'q', 'r','s','t','u','v',\n",
    "       'w','x','y','z',',','-','_','–', ' ']\n",
    "num=['0','1','2','3','4','5','6','7','8','9']\n",
    "dropphraselist=['section', '(', 'usc', 'c. such']\n",
    "dropcateglist=['ORDINAL', 'CARDINAL']\n",
    "dropword1list=[\"the \",\"'\", \"–\"]\n",
    "dropword2list=[\"The \",\"\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropphrase(phrase, catpd):\n",
    "    droplist=[]\n",
    "    #Remove words/phrases containing the string phrase\n",
    "    for ind in catpd.index:\n",
    "        if phrase in catpd.loc[ind, 'WordTextLower'].replace('.',''):\n",
    "            droplist.append(ind)\n",
    "    catpd=catpd.drop(droplist).reset_index().drop(columns=['index'])\n",
    "    return catpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a19b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduper(catpd,dropphraselist,dropcateglist, dropwordlist1):\n",
    "    \n",
    "    #Remove words/phrases containing the strings in dropphraselist\n",
    "    for phrase in dropphraselist:\n",
    "        catpd=dropphrase(phrase, catpd)\n",
    "    \n",
    "    #Remove words/phrases in the categories listed in dropcateglist\n",
    "    for categ in dropcateglist: \n",
    "        catpd=catpd[catpd['CategoryLabel']!=categ]\n",
    "        \n",
    "    #Remove strings from words/phrases in word1list and word2list\n",
    "    for word1 in dropword1list:\n",
    "        for ind in catpd.index:\n",
    "            catpd.loc[ind, 'WordTextLower']=catpd.loc[ind, 'WordTextLower'].replace(word1,'')\n",
    "  \n",
    "    #Remove words with over 95% similarity\n",
    "    droplist=[]\n",
    "    \n",
    "    for ind in catpd.index:\n",
    "        word=catpd.loc[ind, 'WordText']\n",
    "        words=catpd.drop([ind])['WordText'].values\n",
    "        ratio=process.extract(word, words)\n",
    "        for match in ratio:\n",
    "            if match[1] >95 and catpd[catpd['WordText']==match[0]].index[0] not in droplist:\n",
    "            \n",
    "                droplist.append(catpd[catpd['WordText']==match[0]].index[0])\n",
    "    catpd=catpd.drop(droplist)\n",
    "    return catpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c42231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removespecial(catpd):   \n",
    "    droplist=[]\n",
    "    for ind in catpd.index:\n",
    "        word=catpd.loc[ind, 'WordTextLower']\n",
    "        wordcap=catpd.loc[ind, 'WordText']\n",
    "        \n",
    "        #Remove dates with non-numeric values and dates before 1776 and after 2050\n",
    "        if catpd.loc[ind, 'CategoryLabel']=='DATE':\n",
    "            alphayes=0\n",
    "            for let in alpha:\n",
    "                if let in word and ind not in droplist:\n",
    "                    droplist.append(ind)\n",
    "                    alphayes=1\n",
    "            if alphayes==0:\n",
    "                if float(word)>2050 or float(word)<1776:\n",
    "                    droplist.append(ind)\n",
    "        \n",
    "        #Remove section number words (ex. 219c, 333jj)\n",
    "        if len(word)>3:\n",
    "            if len(word)<6 and word[0] in num and word[1] in num and word[2] in num and word[3] in alpha and ind not in droplist:\n",
    "                droplist.append(ind)    \n",
    "        \n",
    "        #Remove leading spaces from word/phrase\n",
    "        word=word.lstrip(' ') \n",
    "        wordcap=wordcap.lstrip(' ') \n",
    "    \n",
    "    catpd=catpd.drop(droplist).reset_index().drop(columns=['index'])\n",
    "    return catpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8132680",
   "metadata": {},
   "outputs": [],
   "source": [
    "catpd=deduper(uniquenounspd_v2,dropphraselist,dropcateglist, dropword1list)\n",
    "catpd=removespecial(catpd)\n",
    "catpd.to_csv(directorybillhead+'ProcessedUniqueNounCategory.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264251ef",
   "metadata": {},
   "source": [
    "# **Complete Sentence/Phrase POS Breakdown Using Unique Nouns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de076ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "# dependency markers for subjects\n",
    "SUBJECTS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
    "# dependency markers for objects\n",
    "OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "# POS tags that will break adjoining items\n",
    "BREAKER_POS = {\"CCONJ\", \"VERB\"}\n",
    "# words that are negations\n",
    "NEGATIONS = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "\n",
    "\n",
    "# does dependency set contain any coordinating conjunctions?\n",
    "def contains_conj(depSet):\n",
    "    return \"and\" in depSet or \"or\" in depSet or \"nor\" in depSet or \\\n",
    "           \"but\" in depSet or \"yet\" in depSet or \"so\" in depSet or \"for\" in depSet\n",
    "\n",
    "\n",
    "# get subs joined by conjunctions\n",
    "def _get_subs_from_conjunctions(subs):\n",
    "    more_subs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if contains_conj(rightDeps):\n",
    "            more_subs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(more_subs) > 0:\n",
    "                more_subs.extend(_get_subs_from_conjunctions(more_subs))\n",
    "    return more_subs\n",
    "\n",
    "\n",
    "# get objects joined by conjunctions\n",
    "def _get_objs_from_conjunctions(objs):\n",
    "    more_objs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if contains_conj(rightDeps):\n",
    "            more_objs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(more_objs) > 0:\n",
    "                more_objs.extend(_get_objs_from_conjunctions(more_objs))\n",
    "    return more_objs\n",
    "\n",
    "\n",
    "# find sub dependencies\n",
    "def _find_subs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verb_negated = _is_negated(head)\n",
    "            subs.extend(_get_subs_from_conjunctions(subs))\n",
    "            return subs, verb_negated\n",
    "        elif head.head != head:\n",
    "            return _find_subs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], _is_negated(tok)\n",
    "    return [], False\n",
    "\n",
    "\n",
    "# is the tok set's left or right negated?\n",
    "def _is_negated(tok):\n",
    "    parts = list(tok.lefts) + list(tok.rights)\n",
    "    for dep in parts:\n",
    "        if dep.lower_ in NEGATIONS:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# get all the verbs on tokens with negation marker\n",
    "def _find_svs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = _get_all_subs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs\n",
    "\n",
    "\n",
    "# get grammatical objects for a given set of dependencies (including passive sentences)\n",
    "def _get_objs_from_prepositions(deps, is_pas):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or (is_pas and dep.dep_ == \"agent\")):\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or\n",
    "                         (tok.pos_ == \"PRON\" and tok.lower_ == \"me\") or\n",
    "                         (is_pas and tok.dep_ == 'pobj')])\n",
    "    return objs\n",
    "\n",
    "\n",
    "# get objects from the dependencies using the attribute dependency\n",
    "def _get_objs_from_attrs(deps, is_pas):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# xcomp; open complement - verb has no suject\n",
    "def _get_obj_from_xcomp(deps, is_pas):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# get all functional subjects adjacent to the verb passed in\n",
    "def _get_all_subs(v):\n",
    "    verb_negated = _is_negated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(_get_subs_from_conjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verb_negated = _find_subs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verb_negated\n",
    "\n",
    "\n",
    "# find the main verb - or any aux verb if we can't find it\n",
    "def _find_verbs(tokens):\n",
    "    verbs = [tok for tok in tokens if _is_non_aux_verb(tok)]\n",
    "    if len(verbs) == 0:\n",
    "        verbs = [tok for tok in tokens if _is_verb(tok)]\n",
    "    return verbs\n",
    "\n",
    "\n",
    "# is the token a verb?  (excluding auxiliary verbs)\n",
    "def _is_non_aux_verb(tok):\n",
    "    return tok.pos_ == \"VERB\" and (tok.dep_ != \"aux\" and tok.dep_ != \"auxpass\")\n",
    "\n",
    "\n",
    "# is the token a verb?  (excluding auxiliary verbs)\n",
    "def _is_verb(tok):\n",
    "    return tok.pos_ == \"VERB\" or tok.pos_ == \"AUX\"\n",
    "\n",
    "\n",
    "# return the verb to the right of this verb in a CCONJ relationship if applicable\n",
    "# returns a tuple, first part True|False and second part the modified verb if True\n",
    "def _right_of_verb_is_conj_verb(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "\n",
    "    # VERB CCONJ VERB (e.g. he beat and hurt me)\n",
    "    if len(rights) > 1 and rights[0].pos_ == 'CCONJ':\n",
    "        for tok in rights[1:]:\n",
    "            if _is_non_aux_verb(tok):\n",
    "                return True, tok\n",
    "\n",
    "    return False, v\n",
    "\n",
    "\n",
    "# get all objects for an active/passive sentence\n",
    "def _get_all_objs(v, is_pas):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS or (is_pas and tok.dep_ == 'pobj')]\n",
    "    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "\n",
    "    #potentialNewVerb, potentialNewObjs = _get_objs_from_attrs(rights)\n",
    "    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "    #    objs.extend(potentialNewObjs)\n",
    "    #    v = potentialNewVerb\n",
    "\n",
    "    potential_new_verb, potential_new_objs = _get_obj_from_xcomp(rights, is_pas)\n",
    "    if potential_new_verb is not None and potential_new_objs is not None and len(potential_new_objs) > 0:\n",
    "        objs.extend(potential_new_objs)\n",
    "        v = potential_new_verb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(_get_objs_from_conjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "\n",
    "# return true if the sentence is passive - at he moment a sentence is assumed passive if it has an auxpass verb\n",
    "def _is_passive(tokens):\n",
    "    for tok in tokens:\n",
    "        if tok.dep_ == \"auxpass\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# resolve a 'that' where/if appropriate\n",
    "def _get_that_resolution(toks):\n",
    "    for tok in toks:\n",
    "        if 'that' in [t.orth_ for t in tok.lefts]:\n",
    "            return tok.head\n",
    "    return None\n",
    "\n",
    "\n",
    "# simple stemmer using lemmas\n",
    "def _get_lemma(word: str):\n",
    "    tokens = nlp(word)\n",
    "    if len(tokens) == 1:\n",
    "        return tokens[0].lemma_\n",
    "    return word\n",
    "\n",
    "\n",
    "# print information for displaying all kinds of things of the parse tree\n",
    "def printDeps(toks):\n",
    "    for tok in toks:\n",
    "        #print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
    "\n",
    "\n",
    "# expand an obj / subj np using its chunk\n",
    "def expand(item, tokens, visited):\n",
    "    if item.lower_ == 'that':\n",
    "        temp_item = _get_that_resolution(tokens)\n",
    "        if temp_item is not None:\n",
    "            item = temp_item\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    if hasattr(item, 'lefts'):\n",
    "        for part in item.lefts:\n",
    "            if part.pos_ in BREAKER_POS:\n",
    "                break\n",
    "            if not part.lower_ in NEGATIONS:\n",
    "                parts.append(part)\n",
    "\n",
    "    parts.append(item)\n",
    "\n",
    "    if hasattr(item, 'rights'):\n",
    "        for part in item.rights:\n",
    "            if part.pos_ in BREAKER_POS:\n",
    "                break\n",
    "            if not part.lower_ in NEGATIONS:\n",
    "                parts.append(part)\n",
    "\n",
    "    if hasattr(parts[-1], 'rights'):\n",
    "        for item2 in parts[-1].rights:\n",
    "            if item2.pos_ == \"DET\" or item2.pos_ == \"NOUN\":\n",
    "                if item2.i not in visited:\n",
    "                    visited.add(item2.i)\n",
    "                    parts.extend(expand(item2, tokens, visited))\n",
    "            break\n",
    "\n",
    "    return parts\n",
    "\n",
    "\n",
    "# convert a list of tokens to a string\n",
    "def to_str(tokens):\n",
    "    if isinstance(tokens, Iterable):\n",
    "        return ' '.join([item.text for item in tokens])\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "# find verbs and their subjects / objects to create SVOs, detect passive/active sentences\n",
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    is_pas = _is_passive(tokens)\n",
    "    verbs = _find_verbs(tokens)\n",
    "    visited = set()  # recursion detection\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = _get_all_subs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            isConjVerb, conjV = _right_of_verb_is_conj_verb(v)\n",
    "            if isConjVerb:\n",
    "                v2, objs = _get_all_objs(conjV, is_pas)\n",
    "                for sub in subs:\n",
    "                    for obj in objs:\n",
    "                        objNegated = _is_negated(obj)\n",
    "                        if is_pas:  # reverse object / subject for passive\n",
    "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
    "                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
    "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
    "                                         \"!\" + v2.lemma_ if verbNegated or objNegated else v2.lemma_, to_str(expand(sub, tokens, visited))))\n",
    "                        else:\n",
    "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
    "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                         \"!\" + v2.lower_ if verbNegated or objNegated else v2.lower_, to_str(expand(obj, tokens, visited))))\n",
    "            else:\n",
    "                v, objs = _get_all_objs(v, is_pas)\n",
    "                for sub in subs:\n",
    "                    if len(objs) > 0:\n",
    "                        for obj in objs:\n",
    "                            objNegated = _is_negated(obj)\n",
    "                            if is_pas:  # reverse object / subject for passive\n",
    "                                svos.append((to_str(expand(obj, tokens, visited)),\n",
    "                                             \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
    "                            else:\n",
    "                                svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                             \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
    "                    else:\n",
    "                        # no obj - just return the SV parts\n",
    "                        svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                     \"!\" + v.lower_ if verbNegated else v.lower_,))\n",
    "\n",
    "    return svos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allwordPOS2(header_gb):\n",
    "    allpd=pd.DataFrame(columns=['BillID', 'HeaderID', 'EntityOrderID', 'Text', 'Sub', 'Verb', 'Pred'])\n",
    "    nlp= spacy.load(\"en_core_web_sm\")\n",
    "    for ind in header_gb.index:\n",
    "        doc = nlp(header_gb.loc[ind,'Text']) \n",
    "        svo=findSVOs(doc)\n",
    "        if len(svo)>0:\n",
    "            if len(svo[0])>1:\n",
    "                if len(svo[0])<3:\n",
    "                    pred='NA'\n",
    "                else:\n",
    "                    pred=svo[0][2] \n",
    "\n",
    "                allpd=allpd.append({ 'BillID': header_gb.loc[ind, 'BillID'], \n",
    "                                    'HeaderID': header_gb.loc[ind, 'HeaderID'], 'Text':doc,\n",
    "                                    'Sub':svo[0][0], 'Verb': svo[0][1], 'Pred':pred},ignore_index=True)\n",
    "    return allpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allwordPOS3(allpd):\n",
    "    droplist=[]\n",
    "    nlp= spacy.load(\"en_core_web_sm\")\n",
    "    allpd['Sub Category']=''\n",
    "    allpd['Pred Category']=''\n",
    "    for ind in allpd.index:\n",
    "        sub=0\n",
    "        pred=0\n",
    "        for wordind in catpd.index:\n",
    "            word=catpd.loc[wordind,'WordTextLower']\n",
    "            if word in allpd.loc[ind,'Sub'].lower():\n",
    "                allpd.loc[ind,'Sub']=word\n",
    "                allpd.loc[ind,'Sub Category']=catpd.loc[wordind,'CategoryLabel']\n",
    "                sub=sub+1\n",
    "            if word in allpd.loc[ind,'Pred'].lower():\n",
    "                allpd.loc[ind,'Pred']=word\n",
    "                allpd.loc[ind,'Pred Category']=catpd.loc[wordind,'CategoryLabel']\n",
    "                pred=pred+1\n",
    "        if len(allpd.loc[ind,'Sub Category'])==0 or len(allpd.loc[ind,'Pred Category'])==0:\n",
    "            droplist.append(ind)\n",
    "    allpd2=allpd.drop(droplist)      \n",
    "    return allpd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb437d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterallpd(allpd2):\n",
    "    droplist=[]\n",
    "   \n",
    "    for ind in allpd2.index:\n",
    "       \n",
    "        if allpd2.loc[ind,'Sub'].lower()=='na' or allpd2.loc[ind,'Pred'].lower()=='na':\n",
    "            droplist.append(ind)\n",
    "        elif '(' in allpd2.loc[ind,'Sub'].lower() or '(' in allpd2.loc[ind,'Pred'].lower():\n",
    "            droplist.append(ind)\n",
    "        elif ')' in allpd2.loc[ind,'Sub'].lower() or ')' in allpd2.loc[ind,'Pred'].lower(): \n",
    "            droplist.append(ind)\n",
    "        elif allpd2.loc[ind,'Verb'].lower()=='is':\n",
    "            droplist.append(ind)\n",
    "        elif 'usc' in allpd2.loc[ind,'Sub'].lower().replace('.','') or 'usc' in allpd2.loc[ind,'Pred'].lower().replace('.',''): \n",
    "            droplist.append(ind)\n",
    "        \n",
    "        allpd2.loc[ind, 'Sub']=allpd.loc[ind, 'Sub'].lower().lstrip( 'a ')\n",
    "        allpd2.loc[ind, 'Pred']=allpd.loc[ind, 'Pred'].lower().lstrip( 'a ')  \n",
    "        allpd2.loc[ind, 'Sub']=allpd.loc[ind, 'Sub'].lower().replace('the', '').replace('of','').replace(' a ', '')\n",
    "        allpd2.loc[ind, 'Pred']=allpd.loc[ind, 'Pred'].lower().replace('the', '').replace('of','').replace(' a ', '')\n",
    "        allpd2.loc[ind, 'Verb']=allpd.loc[ind, 'Verb'].lower().replace('the', '').replace('of','').replace(' a ', '')\n",
    "    allpd3=allpd2.drop(droplist) \n",
    "    return allpd3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f61a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "allpd=allwordPOS2(header_gb)\n",
    "allpd=allpd.reset_index().drop(columns=['index'])\n",
    "allpd2=allwordPOS3(allpd)\n",
    "allpd2=allpd2.reset_index().drop(columns=['index'])\n",
    "allpd3=filterallpd(allpd2)\n",
    "allpd3=allpd3.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9882581",
   "metadata": {},
   "outputs": [],
   "source": [
    "allpd3.to_csv(directorybillhead+'allpd3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allpd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allwordPOS(header_gb):\n",
    "    verbspd=pd.DataFrame(columns=['BillID', 'HeaderID', 'EntityOrderID', 'Text','PartofSpeech', 'SubjObjPred', 'HeadText'])\n",
    "    nlp= spacy.load(\"en_core_web_sm\")\n",
    "    for ind in header_gb.index:\n",
    "        doc = nlp(header_gb.loc[ind,'Text']) \n",
    "        span = doc[doc[len(doc)-1].left_edge.i : doc[len(doc)-1].right_edge.i+1]\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            retokenizer.merge(span)\n",
    "            entorder=0 \n",
    "            for token in doc:\n",
    "                entorder=entorder+1\n",
    "                verbspd=verbspd.append({ 'BillID': header_gb.loc[ind, 'BillID'], \n",
    "                            'HeaderID': header_gb.loc[ind, 'HeaderID'],'EntityOrderID':entorder, 'Text':token.text,\n",
    "                            'PartofSpeech':token.pos_, 'SubjObjPred': token.dep_, 'HeadText':token.head.text},ignore_index=True)\n",
    "    return verbspd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fff83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
